{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82484eb7-5c53-404c-90ea-af3d9c24f946",
   "metadata": {},
   "source": [
    "# Notebook 1: Text representations and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec806acb-3b80-4ba8-8328-1a3ae94f69f7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd55ec-9b6e-4375-88e5-247eea027be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))\n",
    "print(newsgroups[\"DESCR\"][:394])\n",
    "\n",
    "docs, labels, target_names = newsgroups[\"data\"], newsgroups[\"target\"], newsgroups[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ca6a9-6977-4a4e-8f31-217699a5fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8106a7-950f-4d21-bfd3-c7ce54a2e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993736bf-b9fa-470d-be33-31ac280e39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names[labels[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a6774-4721-4f06-bf43-c17dacbd265b",
   "metadata": {},
   "source": [
    "## 1. The bag of words model (BOW)\n",
    "\n",
    "Bag of words model helps convert the text into numerical representation (numerical feature vectors) such that the same can be used to train models using machine learning algorithms. Here are the key steps of fitting a bag-of-words model:\n",
    "\n",
    "- Create a vocabulary indices of words or tokens from the entire set of documents. The vocabulary indices can be created in alphabetical order. \n",
    "- Construct the numerical feature vector for each document that represents how frequent each word appears in different documents. The feature vector representing each will be sparse in nature as the words in each document will represent only a small subset of words out of all words (bag-of-words) present in entire set of documents.\n",
    "\n",
    "Further reading:\n",
    "- https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bfcf2-e8c0-46ac-aa43-7e689aa48a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example_docs = np.array(['Mirabai has won a silver medal in weight lifting in Tokyo olympics 2021',\n",
    "                         'Sindhu has won a bronze medal in badminton in Tokyo olympics',\n",
    "                         'Indian hockey team is in top four team in Tokyo olympics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea1dde-b80e-42d4-8686-d52163f28dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "encoded_example_docs = vectorizer.fit_transform(example_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6edab6-3823-41ee-8a2d-2f0acd326938",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example_docs[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e1f41-adf0-4211-a55a-3756d2a6429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example_docs[0].toarray().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fe302-26b5-4b3e-91fb-ac472f4ce05d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc2f4e-4948-4b9e-aa19-a89b4dfc2836",
   "metadata": {},
   "source": [
    "### 1.1 Tfidf\n",
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms. This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.\n",
    "\n",
    "#### Definition:\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n",
    "\n",
    "$$\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}$$\n",
    "\n",
    "$$\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1$$\n",
    "\n",
    "- $n$ is the total number of documents in the document set\n",
    "- term frequency (tf): the number of times a term occurs in a given document\n",
    "- document frequency of a term $t$ (df(t)):  the number of documents in the document set that contain term $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af37fa6-5613-4303-acfd-c8b7a88b92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "encoded_example_docs = tfidf_vectorizer.fit_transform(example_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b925378-9ba3-4fbe-ad9b-3753950f74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example_docs[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab658d13-d122-49a7-b367-703399f2e515",
   "metadata": {},
   "source": [
    "### 1.2 n-grams\n",
    "The simple BOW-model loses all ordering information of words. To preserve some of the local ordering information we can extract 2-grams (or higher n) of words in addition to the 1-grams (individual words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d00ce-31b9-4b8d-bb87-3500ea8cb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_bigram = CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "encoded_example_docs = vectorizer_bigram.fit_transform(example_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263e93f-bc29-47cf-a8b0-83fe05e99a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example_docs[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559cb38-064f-4942-b4c9-8316d7c0f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bigram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325b138-f8a2-47ba-abef-39825a883c2c",
   "metadata": {},
   "source": [
    "## 2. Language model embeddings\n",
    "Huggingface/sentence bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04191de1-a796-4e96-bc9e-8ef5d9c15b75",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(example_docs.tolist())\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(example_docs, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0cd8c7-4e9d-4a66-9294-7349fc6c9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS SLOW ON CPU\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "# lm_embeddings = model.encode(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e222d72-ba43-4050-8bfd-4b11c316e7b2",
   "metadata": {},
   "source": [
    "## 3. Compare word/document vectors and find similars\n",
    "\n",
    "We can compare word vectors with a similarity. An established measure is the cosine similarity (or the scalar product) of embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f914c-dedd-44ba-804e-45fdd2e26c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(encoded_example_docs[0], encoded_example_docs[1])\n",
    "similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b00d0-00ee-4f53-bcd6-785880eede21",
   "metadata": {},
   "source": [
    "### 3.1 Lets search in the original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e3cd6-f256-4e2d-b365-23ef5b10d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "encoded_docs = tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2772e9d-022e-4b07-a038-833e4b7971d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Why is my Windows PC so slow?\"\n",
    "query_embedding = tfidf_vectorizer.transform([query_text])[0]\n",
    "\n",
    "scores = []\n",
    "for emb in encoded_docs:\n",
    "    score = cosine_similarity(query_embedding, emb)[0][0]\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8364e-08f4-43b4-a159-38f03d5456c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the highest scoring document\n",
    "docs[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a448bd-827f-45aa-ba2c-9e79e14e232a",
   "metadata": {},
   "source": [
    "## 4. Visual exploration\n",
    "\n",
    "Further reading:\n",
    "- UMAP: https://umap-learn.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b277d0-62ed-40d2-b885-7deba73f9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "# Some plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from bokeh.plotting import show, save, output_notebook, output_file\n",
    "from bokeh.resources import INLINE\n",
    "output_notebook(resources=INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4c6ef-db14-466f-a38d-0214e2f239ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=50)\n",
    "pca_embedding = pca.fit_transform(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a4104-9300-4e21-9002-4f88d1dbf324",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(n_components=2, metric='cosine').fit(pca_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970eb20-5b69-456e-9109-cf2c22f1fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df = pd.DataFrame({\n",
    "    \"id\": list(range(len(docs))),\n",
    "    \"label\": [target_names[label] for label in labels]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864d20e-51bd-4366-adf7-2d4b4dbe3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = umap.plot.interactive(embedding, labels=document_df.label, hover_data=document_df, point_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e2ecf-a3da-444e-8962-b48b56876b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
